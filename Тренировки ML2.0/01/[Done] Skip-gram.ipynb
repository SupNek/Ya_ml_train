{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b2e699-b763-477c-89cd-492e2a860f8b",
   "metadata": {},
   "source": [
    "# Word2vec - Skip-Gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083da7fd-3296-4947-bf4e-7ceefcf65ac5",
   "metadata": {},
   "source": [
    "## Необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5d8ad-a8df-486e-a7c3-6cb01a12cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk bokeh umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ff222-ec32-4e18-8b03-248c6f85b4ad",
   "metadata": {},
   "source": [
    "import itertools\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from tqdm.auto import tqdm as tqdma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4486537-165d-443f-9734-fea0bf052006",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6afffc-28a1-4e36-a2ef-a870eefcf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92b93e-d3dd-410e-af3a-efefbfb584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6939c-eb59-4e51-a139-76beff9b0bda",
   "metadata": {},
   "source": [
    "Токенизируем на отдельные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee61a6-868a-40cb-95f6-d93ed9c989ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = [\n",
    "    tokenizer.tokenize(\n",
    "        line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "    )\n",
    "    for line in data\n",
    "] # генератор в котором токенизируем каждое предложение\n",
    "data_tok = [x for x in data_tok if len(x) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b91c1-0a1a-4b67-801e-54033dddd187",
   "metadata": {},
   "source": [
    "Проверки корректности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802e0d8-5fac-42d8-8113-40db034f0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    isinstance(row, (list, tuple)) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(\n",
    "    all(isinstance(tok, str) for tok in row) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all(\"a\" <= x.lower() <= \"z\" for x in tok)\n",
    "assert all(\n",
    "    map(lambda l: not is_latin(l) or l.islower(), map(\" \".join, data_tok))\n",
    "), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a12065-d690-4442-9532-d8955d49bc52",
   "metadata": {},
   "source": [
    "Задаем ширину окна, предобрабатываем и собираем контекстные пары"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b512538-bb1c-4d0b-bf1e-fae8a5be7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "window_radius = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53802f5-5bd6-4bbc-98c2-dee62bee03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_with_counter = Counter(chain.from_iterable(data_tok))\n",
    "\n",
    "word_count_dict = dict()\n",
    "for word, counter in vocabulary_with_counter.items():\n",
    "    if counter >= min_count:\n",
    "        word_count_dict[word] = counter\n",
    "\n",
    "vocabulary = set(word_count_dict.keys())\n",
    "del vocabulary_with_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e20a3-340c-4730-aa9c-6fd9a80fdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocabulary)} # (слово, индекс)\n",
    "index_to_word = {index: word for word, index in word_to_index.items()} # (индекс, слово)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592912b6-f694-4157-8b89-8e55fe4c1ead",
   "metadata": {},
   "source": [
    "Генерируем пары вида `(слово, контекст)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db1117-2999-4b51-8d60-b91fe7c3dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pairs = []\n",
    "\n",
    "for text in data_tok:\n",
    "    for i, central_word in enumerate(text): # выбираем центральное слово\n",
    "        context_indices = range(\n",
    "            max(0, i - window_radius), min(i + window_radius, len(text))\n",
    "        )\n",
    "        for j in context_indices:\n",
    "            if j == i:\n",
    "                continue\n",
    "            context_word = text[j]\n",
    "            if central_word in vocabulary and context_word in vocabulary:\n",
    "                context_pairs.append(\n",
    "                    (word_to_index[central_word], word_to_index[context_word]) # нашли пары разрешенных слов и добавили в массив\n",
    "                )\n",
    "\n",
    "print(f\"Generated {len(context_pairs)} pairs of target and context words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfdc04-92e6-4873-90e5-7b6443e232c6",
   "metadata": {},
   "source": [
    "### Подзадача №1: subsampling\n",
    "Для того, чтобы сгладить разницу в частоте встречаемсости слов, необходимо реализовать механизм subsampling'а.\n",
    "Для этого вам необходимо реализовать функцию ниже.\n",
    "\n",
    "Вероятность **оставить** слово из обучения (на фиксированном шаге) вычисляется как\n",
    "$$\n",
    "P_\\text{save}(w_i)=\\sqrt{\\frac{t}{f(w_i)}},\n",
    "$$\n",
    "где $f(w_i)$ – нормированная частота встречаемости слова, а $t$ – заданный порог (threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b648a9-6a1e-49dc-a250-8343a4267a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frequent_words(word_count_dict, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Calculates the subsampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function is used to determine the probability of keeping a word in the dataset\n",
    "    when subsampling frequent words. The method used is inspired by the subsampling approach\n",
    "    in Word2Vec, where each word's frequency affects its probability of being kept.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "    - threshold (float, optional): A threshold parameter used to adjust the frequency of word subsampling.\n",
    "                                   Defaults to 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of keeping each word.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # import numpy as np\n",
    "    \n",
    "    all_w_count = sum(word_count_dict.values())\n",
    "    freq = {word: word_count_dict[word] / all_w_count for word in word_count_dict}\n",
    "    prob = {word: (threshold / freq[word]) ** 0.5 for word in freq}\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e54d1f-c22a-43f6-9b6d-b2e8882a8372",
   "metadata": {},
   "source": [
    "### Подзадача №2: negative sampling\n",
    "Для более эффективного обучения необходимо не только предсказывать высокие вероятности для слов из контекста, но и предсказывать низкие для слов, не встреченных в контексте. Для этого вам необходимо вычислить вероятност использовать слово в качестве negative sample, реализовав функцию ниже.\n",
    "\n",
    "В оригинальной статье предлагается оценивать вероятность слов выступать в качестве negative sample согласно распределению $P_n(w)$\n",
    "$$\n",
    "P_n(w) = \\frac{U(w)^{3/4}}{Z},\n",
    "$$\n",
    "\n",
    "где $U(w)$ распределение слов по частоте (или, как его еще называют, по униграммам), а $Z$ – нормировочная константа, чтобы общая мера была равна $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db4829-bdf1-446c-9206-14edb9ffb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_prob(word_count_dict):\n",
    "    \"\"\"\n",
    "    Calculates the negative sampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function adjusts the frequency of each word raised to the power of 0.75, which is\n",
    "    commonly used in algorithms like Word2Vec to moderate the influence of very frequent words.\n",
    "    It then normalizes these adjusted frequencies to ensure they sum to 1, forming a probability\n",
    "    distribution used for negative sampling.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of selecting each word\n",
    "            for negative sampling.\n",
    "    \"\"\"\n",
    "    all_w_count = sum(word_count_dict.values())\n",
    "    freq = {word: (word_count_dict[word] / all_w_count) ** 0.75 for word in word_count_dict}\n",
    "    Z = sum(freq.values())\n",
    "    return {word: freq[word] / Z for word in freq}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb17d0b-8970-4939-b61b-780ee2cfdbe5",
   "metadata": {},
   "source": [
    "### Формирование словарей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386632aa-d420-44ed-ba2f-3dba34b69fac",
   "metadata": {},
   "source": [
    "Для удобства, преобразуем полученные словари в массивы (т.к. все слова все равно уже пронумерованы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ec934-33cf-4db8-b31c-787d1e9e60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_dict = subsample_frequent_words(word_count_dict)\n",
    "assert keep_prob_dict.keys() == word_count_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82fc92-69ab-4091-9550-9f56630bb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\n",
    "assert negative_sampling_prob_dict.keys() == negative_sampling_prob_dict.keys()\n",
    "assert np.allclose(sum(negative_sampling_prob_dict.values()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a23a9-e422-41a8-9912-b2795e388820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# полученные массивы\n",
    "keep_prob_array = np.array(\n",
    "    [keep_prob_dict[index_to_word[idx]] for idx in range(len(word_to_index))]\n",
    ")\n",
    "negative_sampling_prob_array = np.array(\n",
    "    [\n",
    "        negative_sampling_prob_dict[index_to_word[idx]]\n",
    "        for idx in range(len(word_to_index))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e178a4-a329-4ce8-b920-fc250fa551af",
   "metadata": {},
   "source": [
    "## Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153aa2fe-9a1b-4858-a9ab-099bb73b9f16",
   "metadata": {},
   "source": [
    "Наконец, время реализовать модель.\n",
    "\n",
    "Напомним, что в случае negative sampling решается задача максимизации следующего функционала:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log \\sigma({\\mathbf{v}'_{w_O}}^\\top \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma({-\\mathbf{v}'_{w_i}}^\\top \\mathbf{v}_{w_I}) \\right],\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{v}_{w_I}$ – вектор центрального слова $w_I$,\n",
    "- $\\mathbf{v}'_{w_O}$ – вектор слова из контекста $w_O$,\n",
    "- $k$ – число negative samples,\n",
    "- $P_n(w)$ – распределение negative samples, заданное выше,\n",
    "- $\\sigma$ – сигмоида."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59656e-feba-4c1d-9a0c-6339f65bafe7",
   "metadata": {},
   "source": [
    "Далее по ходу работы будем использовать лосс: nn.BCEWithLogitsLoss(), в котором уже реализовано почти все, остается посчитать только произведения векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde30b8-c32a-413d-8bc6-58d5e5e4f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModelWithNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModelWithNegSampling, self).__init__()\n",
    "        self.embeddings_in = nn.Embedding(vocab_size, embedding_dim) # center\n",
    "        self.embeddings_out = nn.Embedding(vocab_size, embedding_dim) # context\n",
    "        \n",
    "        # никакая логсигмоида нам не нужна! это все заложено в лоссе\n",
    "        torch.nn.init.xavier_uniform_(self.embeddings_in.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.embeddings_out.weight)\n",
    "        \n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        # center_words - входные слова\n",
    "        # pos_context_words - таргет, т.е. правильный контекст (реально существующий для входного слова)\n",
    "        # слова которые ТОЧНО не будут в контексте\n",
    "\n",
    "        v_in = self.embeddings_in(center_words) \n",
    "        v_out = self.embeddings_out(pos_context_words)\n",
    "        v_neg = self.embeddings_out(neg_context_words)\n",
    "        \n",
    "        pos_scores = (torch.sum(v_in * v_out, dim=1))\n",
    "        neg_scores = (torch.bmm(v_neg, v_in.unsqueeze(2)).squeeze(2)) #.sum(1) # bmm - батчевое (по 2D-матричное) перемножение матриц\n",
    "        return pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14679140-c3ed-4224-b08d-a07db11ca9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") # TODO: реализовать перенос на cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddfb0e-a436-469a-9738-d19e057675af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = Word2VecDataset(context_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2bc41-cdda-43a0-af93-7006c33cce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 32\n",
    "num_negatives = 15\n",
    "\n",
    "model = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05) # оптимизатор параметров методом Adam\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=30) # штука, которая будет уполовинивать (* factor = 1/2) lr при отсутствии улучшений в течение 150 эпох\n",
    "criterion = nn.BCEWithLogitsLoss() # тот самый лосс, похож на логлосс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cce8e6-3c9c-49bc-aae3-34def989723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем корректность параметров модели (nn.Linear не нужны)\n",
    "params_counter = 0\n",
    "for weights in model.parameters():\n",
    "    params_counter += weights.shape.numel()\n",
    "assert params_counter == len(word_to_index) * embedding_dim * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c08107-e452-49ed-8a9b-150fdfa6b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(\n",
    "    n_smpl,\n",
    "    negative_sampling_prob_array,\n",
    "    num_negatives,\n",
    "):\n",
    "    return np.random.choice(\n",
    "                range(n_smpl),\n",
    "                size=num_negatives,\n",
    "                p=negative_sampling_prob_array,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8727b84-8f69-4d37-9c38-ef15d2380cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    epohs,\n",
    "    steps,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "    \n",
    "):\n",
    "    pos_labels = torch.ones(batch_size).to(device)\n",
    "    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n",
    "    loss_history = []\n",
    "    step = 0\n",
    "    n_smpl = len(negative_sampling_prob_array)\n",
    "    for epoh in tqdma(range(epohs)):\n",
    "        if step > steps:\n",
    "            break\n",
    "        for target, context in dataloader:\n",
    "            if step > steps:\n",
    "                break\n",
    "            center_words = target.long() # \n",
    "            pos_context_words = context.long()\n",
    "            neg_context_words = torch.LongTensor(np.array([get_negative_samples(n_smpl, negative_sampling_prob_array, num_negatives) for t in center_words]))\n",
    "            optimizer.zero_grad()\n",
    "            pos_scores, neg_scores = model(\n",
    "                center_words, pos_context_words, neg_context_words\n",
    "            )\n",
    "            loss_pos = criterion(pos_scores, pos_labels)\n",
    "            loss_neg = criterion(neg_scores, neg_labels)\n",
    "    \n",
    "            loss = loss_pos + loss_neg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            loss_history.append(loss.item())\n",
    "            lr_scheduler.step(loss_history[-1])\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(\n",
    "                    f\"Step {step}, Loss: {np.mean(loss_history[-10:])}, learning rate: {lr_scheduler._last_lr}\"\n",
    "                )\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e91096-5413-4ead-b9ac-7809e536cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 300\n",
    "batch_size = 5000\n",
    "epohs = 5\n",
    "train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    epohs,\n",
    "    steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f088e2-4f05-4199-965d-16184b00d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a02862-6699-41df-a5c4-f11c7652d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, embedding_matrix, word_to_index=word_to_index):\n",
    "    return embedding_matrix[word_to_index[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e91c3e-1c2c-4035-a23f-7159541ea9b6",
   "metadata": {},
   "source": [
    "Проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35948ac6-389d-46d9-9da3-4c2a5d0e578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"apple\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"dell\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afe214-d51b-400c-846f-d7920ea1a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"laptop\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"macbook\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b289bbd-0e9d-4a7d-a60d-10c9be94ed8a",
   "metadata": {},
   "source": [
    "Посмотрим на ближайщие слова по косинусной мере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc41d5-0486-454b-9635-2b2c134dd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word, embedding_matrix, word_to_index=word_to_index, k=10):\n",
    "    word_vector = get_word_vector(word, embedding_matrix)[None, :]\n",
    "    dists = F.cosine_similarity(embedding_matrix, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-k:]\n",
    "    return [(index_to_word[x], dists[x].item()) for x in top_k.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2295830-ff48-45eb-8b7d-f2c93c54ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(\"python\", embedding_matrix_context, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda271f9-a1d6-4793-8a2e-515fb6a4586f",
   "metadata": {},
   "source": [
    "Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29435dca-0355-438c-acd9-836ec8fd0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "_top_words = sorted([x for x in word_count_dict.items()], key=lambda x: x[1])[\n",
    "    -top_k - 100 : -100\n",
    "]  # ignoring 100 most frequent words\n",
    "top_words = [x[0] for x in _top_words]\n",
    "del _top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b02bc-64c2-406b-9d3f-21ae3be9c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.cat(\n",
    "    [embedding_matrix_context[word_to_index[x]][None, :] for x in top_words], dim=0\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5adae-6c9c-43ea-b1ec-72e584cba28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm\n",
    "import bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def draw_vectors(\n",
    "    x,\n",
    "    y,\n",
    "    radius=10,\n",
    "    alpha=0.25,\n",
    "    color=\"blue\",\n",
    "    width=600,\n",
    "    height=400,\n",
    "    show=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"draws an interactive plot for data points with auxilirary info on hover\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({\"x\": x, \"y\": y, \"color\": color, **kwargs})\n",
    "\n",
    "    fig = pl.figure(active_scroll=\"wheel_zoom\", width=width, height=height)\n",
    "    fig.scatter(\"x\", \"y\", size=radius, color=\"color\", alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show:\n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2a96a-d32b-4020-9949-90ad5991e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_neighbors=5).fit_transform(word_embeddings)\n",
    "draw_vectors(embedding[:, 0], embedding[:, 1], token=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213c540-be00-4876-b4df-98e9755325e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import json\n",
    "\n",
    "assert os.path.exists(\n",
    "    \"words_subset.txt\"\n",
    "), \"Please, download `words_subset.txt` and place it in the working directory\"\n",
    "\n",
    "with open(\"words_subset.txt\") as iofile:\n",
    "    selected_words = iofile.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "def get_matrix_for_selected_words(selected_words, embedding_matrix, word_to_index):\n",
    "    word_vectors = []\n",
    "    for word in selected_words:\n",
    "        index = word_to_index.get(word, None)\n",
    "        vector = [0.0] * embedding_matrix.shape[1]\n",
    "        if index is not None:\n",
    "            vector = embedding_matrix[index].numpy().tolist()\n",
    "        word_vectors.append(vector)\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "word_vectors = get_matrix_for_selected_words(\n",
    "    selected_words, embedding_matrix_context, word_to_index\n",
    ")\n",
    "\n",
    "with open(\"submission_dict.json\", \"w\") as iofile:\n",
    "    json.dump(word_vectors, iofile)\n",
    "print(\"File saved to `submission_dict.json`\")\n",
    "# __________end of block__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
